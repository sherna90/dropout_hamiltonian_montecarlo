%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CRoNe ARTICLE TEMPLATE
%%% Based on eLife article template from
%%% https://www.overleaf.com/latex/templates/elife-latex-template/csqxykvsnyxm#.WxxIlxzathE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% PREAMBLE 
\documentclass[9pt,lineno]{crone}
% Use the onehalfspacing option for 1.5 line spacing
% Use the doublespacing option for 2.0 line spacing
% Please note that these options may affect formatting.
% Additionally, the use of the \newcommand function should be limited.
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm} 
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{url}
\usepackage[utf8]{inputenc}
\usepackage{lipsum} % Required to insert dummy text
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
\usepackage{tikz}
\DeclareSIUnit\Molar{M}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% ARTICLE SETUP
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{A Comparison of Stochastic Gradient MCMC using Multi-Core and GPU Architectures}

\author[1*]{Sergio Hern\'andez}
\author[1]{Jos\'e Vald\'es}
\affil[1]{Laboratorio de Procesamiento de Informaci\'on Geoespacial. Universidad Cat\'olica del Maule. Chile}
\affil[2]{Centro de Innovaci\'on en Ingenier\'ia Aplicada.}
\corr{shernandez@ucm.cl}{SH}


%\presentadd[\authfn{1}]{Department, Institute, Country}
%\presentadd[\authfn{2}]{Department, Institute, Country}

\begin{document}

\maketitle

\begin{abstract}
Deep learning models are traditionally used in big data scenarios. When there is not enough training data to fit a large model, transfer learning repurpose the learned features from an existing model and re-train the lower layers for the new task. Bayesian inference techniques can be used to capture the uncertainty of the new model but it comes with a high computational cost. In this paper, we compare the run time performance of an Stochastic Gradient Markov Chain Monte Carlo method using different architectures. As opposed to the widely usage of GPUs for deep learning, we found significant advantages from using modern CPU architectures.
\end{abstract}

\section{Introduction}
As the amount of stored data is increased. analytical techniques for processing large-scale datasets are also required. In the other hand, big datasets are not only described by the number of observations but also through its dimensionality (number of features). Therefore. modern scalable Bayesian inference techniques must be able to improve performance on a single machine (scale up) as well as distributing performance across several machines or CPU cores (scale out) \cite{angelino2016patterns}.

In particular. Bayesian inference is used for computing posterior distributions of the model parameters. In order to obtain such posterior estimates. Markov Chain Monte Carlo (MCMC) methods approximate intractable integrals as an expectation using a finite number of samples. However, this expectation requires the full dataset in order to guarantee convergence.  Splitting a big dataset into smaller batches and running parallel chains is a simple alternative for scaling out Bayesian inference. Nevertheless, each chain will have its own posterior approximation and there is no standard method for recombining the results into a single posterior distribution. 

The Stochastic Gradient Langevin Monte Carlo (SGLD) algorithm is an approximation technique that fully exploits data subsets \cite{welling2011bayesian}. The method replaces gradients and likelihood evaluations required by standard MCMC by stochastic gradients based on data subsets (mini-batches). Some implementations can be found in software libraries such as Edward \cite{Tran2018}, ZhuSuan \cite{zhusuan2017} and the SGMCMC package \cite{baker2017sgmcmc}. All of these implementations are based on TensorFlow\footnote{\url{https://www.tensorflow.org/}} as a backend and therefore inherit the scalability of GPU computation. However, there is little research done on the scalability of SGLD using modern CPU architectures. Therefore, in this paper we present a novel comparison of SGLD implementations on GPUs and CPUs. 
% 
\section{Stochastic Gradient MCMC}
Stochastic Gradient Descent (SGD) is an optimization technique traditionally used for out-of-core training deep learning models. For example, if we represent a dataset $\mathbf{D}=\{\mathbf{d_1},\ldots,\mathbf{d_N}\}$ as a set of tuples $\mathbf{d}=(\mathbf x,y)$ that contains features $\mathbf x \in \mathbb R^D$ and the labels $y=\{1,\ldots,K\}$. An stochastic optimization procedure iterates  over mini-batches of size $B \ll N$ and compute a point estimate $\theta^\ast = \operatorname{argmax} p(\mathbf{D} \vert \theta)$.

Deep learning models such as convolutional neural networks may exploit model parallelism for fully connected layers and data parallelism for convolutional layers. The former training technique applies parallelism across the model dimension (number of features). The latter technique applies parallelism across the data dimension (number of examples). In a model parallel setting, different workers are trained on the same batch and they recombine results as soon as possible. In the other hand, in a data parallel setting, each worker is trained on different batches of data. 

In the Bayesian framework, we consider the unknown parameter $\theta$ as a random variable. The Stochastic Gradient Langevin Monte Carlo (SGLD) algorithm uses an stochastic gradient approximation to generate samples from the posterior distribution $p(\theta \vert \mathbf{D})$. The SGLD algorithm generate proposals using:

\begin{align}
\theta_{t+1}=\theta_{t}+\frac{\epsilon_t}{2}\Big(\nabla\operatorname{log}p(\theta_{t})+\frac{N}{B} \sum_i^B \nabla \operatorname{log} p(\mathbf{d_i} \vert \theta_{t})\Big) + \eta_t
\label{eq:sgld}
\end{align}

where $\eta_t \sim \mathcal N(0,\epsilon_t)$ and $\epsilon_t \mapsto 0$ is a time-decaying step size.

It is important to notice that using SGLD on a fully connected layer requires dense matrix multiplications for computing the log-likelihood and the log-prior. Therefore, we present two implementations using two different architectures.  
    
\subsection{GPU SG-MCMC}
Graphic Processing Units (GPus) are many-core processors that offer massively parallel computation. GPUs are well suited for data parallel operations such as computing the gradient in convolutional neural networks and Monte Carlo methods \cite{lee2010utility}. In order to implement the SGLD algorithm we use CuPy, a Python library for GPU computation\footnote{\url{https://cupy.chainer.org/}}. The library includes optimized GPU array manipulation and linear algebra routines, which are compatible with other CPU array manipulation routines such as Numpy. 
 
 
\begin{figure}
	\centering
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/cpu_architechture}
		\caption{CPU implementation.}
		\label{fig:cpuarchitechture}
	\end{subfigure}
	~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
	%(or a blank line to force the subfigure onto a new line)
	\begin{subfigure}[b]{0.4\textwidth}
		\includegraphics[width=\textwidth]{figures/gpu_architechture}
		\caption{GPU implementation.}
		\label{fig:gpuarchitechture}
	\end{subfigure}
	\caption{SG-MCMC implementation on CPU and GPU. In CPU, a data queue feeds multiple Markov chains. In GPU, a single mini-batch is fed to a Markov chain that updates model parameters.}\label{fig:sgld_architechture}
\end{figure}
 
 
\subsection{CPU SG-MCMC}
Modern CPUs are equipped with multiple cores which are well suited to single instruction multiple data operations. Python is built around a Global Interpreter Lock, which prevents the execution of multiple threads at once.  Nevertheless, parallel Markov can be spawn using all available cores.  Moreover, data parallelism can be achieved by sharing a data queue that feeds each process with a different data mini-batch.  Figure \ref{fig:sgld_architechture} depicts the proposed GPU and CPU implementations of the SGLD algorithm. 


\section{Experimental Results}
In this section, the goal is to demonstrate performance and scalability of the SGLD algorithm. The GPU and CPU implementations are tested using randomly generated multi-class classification problems. Different combinations on the number of features $D$ and number of instances $N$ were tested. The number of classes was set to $K=3$, the number of epochs was set to $E=2 \times 10^4$ and the size of the mini-batches was set to $B=50$ for all examples.

Firstly, a serial version  of the SGD algorithm is tested on the CPU and compared with a parallel implementation on the GPU. All experiments are run on a Server with an Intel Xeon E5-2620 CPU with 12 cores using a single fully connected softmax layer (which is widely used in transfer learning). The server is also equipped with an NVIDIA TITAN X GPU and both implementations were developed using Python 3.4.9. Table \ref{tab:sgd} shows the run time for the different values of $D$ and $N$. According to \cite{LI201695}, convolutional layers contain $90\%$ of the computation and $5\%$ of the parameters, while fully connected layers contain $95\%$ of the parameters and $5\%-10\%$ of the computation. Consistently, since there is a fully connected layer operating on a small batch of data, there is no speedup and the GPU implementation is about $10$x slower than the CPU implementation.

\begin{table}[h]
	\centering
 \begin{tabular}{|c|c|c|c|c|c|c|}
 	\hline 
 	D & N & CPU Time [s] & GPU Time [s] & Name & Name & Speed-Up\\ 
 	\hline 
 	10 &	1000&	11.98&	124.85& 0.0119& 0.1248& 10.42\\
 	10&	10000&	119.90&	1245.98& 0.0119& 0.1245& 10.39\\
 	10&	50000&	594.71&	6186.05& 0.0118& 0.1237& 10.40\\
 	10&	100000&	1195.62&	12261.71& 0.0119& 0.1226& 10.25\\
 	\hline
 	50&	1000&	13.67&	123.13& 0.0136& 0.1231& 9.00\\
 	50&	10000&	135.96&	1230.19& 0.0135& 0.1230& 9.04\\
 	50&	50000&	689.89&	6137.79& 0.0137& 0.1227& 8.89\\
 	50&	100000&	1487.26&	12240.06& 0.0148& 0.1224& 8.22\\
 	\hline
 	100&	1000&	15.71&	123.53& 0.0157& 0.1235& 7.86\\
 	100	&10000&	156.94&	1231.12& 0.0156& 0.1231& 7.84\\
 	100&	50000&	806.78&	6176.91& 0.0161& 0.1235& 7.65\\
 	100&	100000&	1770.83&	12394.13& 0.0177& 0.1239& 6.99\\
 	\hline 
 \end{tabular}
\caption{Run time comparison of CPU and GPU implementations of SGD for a softmax regression problem}
\label{tab:sgd} 
\end{table}
 
Secondly, the run time behavior of a multi-core CPU implementation of SGLD is shown in Table \ref{tab:sgld}. Similar to SGD, run time is scales with the number of data points $N$ and features $D$. This is not the case of the GPU implementation of SGD, where the number of features seems not to affect the run time due to the massive parallel processing capabilities of the GPU.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline 
		D & N & CPU Time [s] & GPU Time [s] & Name & Name & Speed-Up\\ 
		\hline 		
10	& 1000& 	24.75& 294.04  & 0.0247& 0.2940& 11.87\\
10	& 10000& 	245.22& 2946.67	& 0.0245& 0.2946& 12.01\\
10	& 50000& 	1186.81& 14518.66 & 0.0237& 0.2903& 12.23\\
10	& 100000	& 2380.18& 29010.43	& 0.0238& 0.2901& 12.18\\
\hline
50	& 1000	& 25.70& 288.02	& 0.0250& 0.2880& 11.48\\
50	& 10000	& 246.64& 2885.16	& 0.0246& 0.2880& 11.69\\
50	& 50000	& 1230.02& 14467.73	& 0.0246& 0.2893& 11.76\\
50 & 	100000	& 2496.15& 28930.81	& 0.0249& 0.2893& 11.59\\
\hline
100	& 1000	& 30.95& 288.21	& 0.0309& 0.2882& 9.31\\
100	& 10000	& 279.28& 2903.20& 0.0279& 0.2903& 10.39\\
100	& 50000	& 1450.57& 14531.93& 0.0290& 0.2906& 10.01\\
100	& 100000& 	2796.20& 29117.73& 0.0279& 0.2911& 10.41\\

		\hline 
\end{tabular}
\caption{Run time comparison of CPU implementation of SGLD for a softmax regression problem}
\label{tab:sgld} 
\end{table}
\bibliographystyle{unsrt}
\bibliography{biblio}
\end{document}
