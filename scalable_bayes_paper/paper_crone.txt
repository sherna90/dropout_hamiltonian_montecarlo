Proceedings of the 4th Congress on Robotics and Neuroscience

1

2

3

4

*For correspondence:
shernandez@ucm.cl (SH)

A Comparison of Stochastic Gradient
MCMC using Multi-Core and GPU
Architectures
Sergio Hernández1* and José Valdés1

5

1 Laboratorio

6

Maule. Chile

de Procesamiento de Información Geoespacial. Universidad Católica del

7

8
9
10
11
12
13
14

Abstract Deep learning models are traditionally used in big data scenarios. When there is not
enough training data to ﬁt a large model, transfer learning repurpose the learned features from an
existing model and re-train the lower layers for the new task. Bayesian inference techniques can be
used to capture the uncertainty of the new model but it comes with a high computational cost. In
this paper, we compare the run time performance of an Stochastic Gradient Markov Chain Monte
Carlo method using different architectures. As opposed to the widely usage of GPUs for deep
learning, we found signiﬁcant advantages from using modern CPU architectures.

15

16
17
18
19
20
21
22
23
24

25
26
27

28
29

30
31
32

33
34

Introduction
The Stochastic Gradient Langevin Monte Carlo (SGLD) algorithm is an approximation technique
that fully exploits data subsets [4]. The method replaces gradients and likelihood evaluations
required by standard MCMC by stochastic gradients based on data subsets (mini-batches). Some
implementations can be found in software libraries such as Edward [3], ZhuSuan [2] and the
SGMCMC package [1]. All of these implementations are based on TensorFlow1 as a backend and
therefore inherit the scalability of GPU computation. However, there is little research done on the
scalability of SGLD using modern CPU architectures. Therefore, in this paper we present a novel
comparison of SGLD implementations on GPUs and CPUs.

References

[1] Baker J, Fearnhead P, Fox EB, Nemeth C. sgmcmc: An R package for stochastic gradient Markov chain Monte
Carlo. arXiv preprint arXiv:171000578. 2017; .
[2] Shi J, Chen J, Zhu J, Sun S, Luo Y, Gu Y, Zhou Y. ZhuSuan: A Library for Bayesian Deep Learning. arXiv preprint
arXiv:170905870. 2017; .
[3] Tran D, Hoffman MW, Moore D, Suter C, Vasudevan S, Radul A. Simple, Distributed, and Accelerated
Probabilistic Programming. In: Bengio S, Wallach H, Larochelle H, Grauman K, Cesa-Bianchi N, Garnett R,
editors. Advances in Neural Information Processing Systems 31 Curran Associates, Inc.; 2018.p. 7598–7609.
[4] Welling M, Teh YW. Bayesian learning via stochastic gradient Langevin dynamics. In: Proceedings of the 28th
international conference on machine learning (ICML-11); 2011. p. 681–688.

1 https://www.tensorﬂow.org/

